#!venv/bin/python
# uses: openai

# import modules
from openai import OpenAI
import subprocess

# define the model settings
system_prompt = "Be concise and friendly."
TOKENS = 200
MODEL = "gpt-3.5-turbo"
TEMPERATURE = 1

# function to generate a chat completion using the OpenAI API
def chat_completion(
    _system_prompt=system_prompt,
    _user_prompt=None,
    _tokens=TOKENS,
    _model=MODEL,
    _temperature=TEMPERATURE
    ):

    client = OpenAI()
    messages = [
        {"role": "system", "content": _system_prompt},
        {"role": "user", "content": _user_prompt},
    ]

    response = client.chat.completions.create(
        temperature=_temperature,
        max_tokens=_tokens,
        model=_model,
        messages=messages
    )

    content = response.choices[0].message.content
    completion_tokens = response.usage.completion_tokens
    prompt_tokens = response.usage.prompt_tokens
    total_tokens = response.usage.total_tokens

    return {
        "content": content,
        "completion_tokens": completion_tokens,
        "prompt_tokens": prompt_tokens,
        "total_tokens": total_tokens
    }

# function to run rofi
def rofi_run():
    rofi_command = ['rofi', '-dmenu', '-p', 'ðŸ¤–', '-theme', '~/.config/rofi/themes/rofi-gpt']
    result = subprocess.run(rofi_command, capture_output=True, text=True).stdout.strip()
    return result


if __name__ == "__main__":
    user_prompt = rofi_run()
    if not user_prompt:
        print("User canceled. Exiting.")
    else:
        print(user_prompt)
        response = chat_completion(_user_prompt=user_prompt)
        output = response["content"]
        print(output)
        dunstify_command = ['dunstify', output, '-a', 'rofi-gpt']
        subprocess.run(dunstify_command)
